{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satishsampath/photo-genie/blob/main/photo_genie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demo takes a \n",
        "reuses a lot from ShriramShrirao's [excellent Dreambooth tutorial](https://colab.research.google.com/github/ShivamShrirao/diffusers/blob/main/examples/dreambooth/DreamBooth_Stable_Diffusion.ipynb). Thank you!"
      ],
      "metadata": {
        "id": "K2fHtdcL4rv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before you begin - get your photos ready in Google Drive"
      ],
      "metadata": {
        "id": "7xZwvohZ6VKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demo trains a DreamBooth model using your own photos. Since Google Colab can't directly access your Google Photos, you have to upload a .zip file with your photos to Google Drive and connect that to this Colab. Here are the steps.\n",
        "\n",
        "#### Export a set of photos from your Google Photos album. \n",
        "For example, I used just the photos I took in 2022 & 2023.\n",
        "1. Open https://takeout.google.com/. \n",
        "2. Click “Deselect All”\n",
        "3. Scroll down to “Google Photos” and select that row. \n",
        "4. Click the “All photo albums included” button. If it’s not there, wait for a few seconds as it’ll be gathering album info before showing up.\n",
        "5. Clicking that button opens up a modal dialog titled “Google Photos content options”. Click “Deselect All” first, then scroll to “Photos from 2023” (or whichever year(s) you want to use) and select them. Then click “OK” to dismiss this modal.\n",
        "6. Scroll to the very end of the Takeout page and click “Next Step”.\n",
        "7. Select “Send download link via email”, “Export Once”, “.zip” and “2 GB” as the options. Then click “Create Export”.\n",
        "8. Wait for a few minutes until you receive a download link in your google mail. Click on the link in the email to download the zip file containing all your Google Photos data for the selected year(s).\n",
        "\n",
        "The .zip files contain videos & photos together. Uploading videos takes a very long time and they are not used, so remove them before uploading.\n",
        "1. Extract the .zip file to a local folder\n",
        "2. Delete all the .mp4 files\n",
        "3. Create a new .zip file with the remaining downloaded files.\n",
        "\n",
        "Open your Google Drive folder in the browser and upload the zip file. The Colab notebook will access it from there.\n",
        "1. Open https://drive.google.com  and open the “Colab Notebooks” folder in there.\n",
        "2. Create a new `photo_genie` folder\n",
        "3. Upload your photos .zip files to this folder\n"
      ],
      "metadata": {
        "id": "ZHMAsnwF59z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "dojwjuRRRIk6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAcS6rg_HjXp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install the required libs\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers\n",
        "%pip install -qq \"ipywidgets>=7,<8\"\n",
        "%pip install -qq deepface\n",
        "%pip install -qq opencv-python\n",
        "%pip install -qq matplotlib\n",
        "%pip install -qq pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_h0kO-VnQog",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Initialize imports & global functions\n",
        "import argparse\n",
        "import itertools\n",
        "import math\n",
        "from contextlib import nullcontext\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import set_seed\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from deepface import DeepFace\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import re\n",
        "import shutil\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import zipfile\n",
        "\n",
        "from deepface import DeepFace\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import re\n",
        "import shutil\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import zipfile\n",
        "\n",
        "\"\"\" Given a directory with .zip files, extracts .jpg or .jpeg files from them all to a specified directory. \"\"\"\n",
        "def extract_photos_from_zips(zips_dir, dest_dir):\n",
        "  os.makedirs(dest_dir, exist_ok=True)\n",
        "  files = [file for file in os.listdir(zips_dir) if file.endswith('.zip')]\n",
        "  for file in files:\n",
        "    zip_path = os.path.join(zips_dir, file)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "      for name in zip_ref.namelist():\n",
        "        if name.lower().endswith(('.jpg', '.jpeg')):\n",
        "          with zip_ref.open(name) as src_file:\n",
        "            dst_file_path = os.path.join(dest_dir, os.path.basename(name))\n",
        "            print(os.path.basename(name), end=\" \")\n",
        "            with open(dst_file_path, 'wb') as dst_file:\n",
        "              dst_file.write(src_file.read())\n",
        "  print(\"\\n\")\n",
        "\n",
        "\"\"\"Find unique faces based on the given embedding vectors and threshold.\"\"\"\n",
        "def find_unique_faces_dbscan(embeddings, threshold=0.6):\n",
        "  embeddings_array = np.array([embedding for _, embedding in embeddings])\n",
        "\n",
        "  # Normalize the embedding vectors\n",
        "  scaler = StandardScaler()\n",
        "  embeddings_array = scaler.fit_transform(embeddings_array)\n",
        "\n",
        "  # Perform DBSCAN clustering on the embedding vectors\n",
        "  dbscan = DBSCAN(metric='cosine', eps=threshold, min_samples=5)\n",
        "  labels = dbscan.fit_predict(embeddings_array)\n",
        "\n",
        "  unique_labels = list(set(labels))\n",
        "  unique_faces = []\n",
        "  for label in unique_labels:\n",
        "    # Find the indices of embeddings belonging to the current label\n",
        "    indices = np.where(labels == label)[0]\n",
        "    item = {'count': len(indices), 'photos':[], 'embeddings':[]}\n",
        "    for i in indices:\n",
        "      item['photos'].append(embeddings[i][0])\n",
        "    unique_faces.append(item)\n",
        "\n",
        "  return unique_faces\n",
        "\n",
        "\"\"\"Generate embedding vectors for face images in a folder matching a given pattern.\"\"\"\n",
        "def generate_embedding_vectors(folder_name, pattern):\n",
        "  embedding_vectors = []\n",
        "  file_names = [file for file in os.listdir(folder_name) if re.match(pattern, file)]\n",
        "\n",
        "  for file_name in file_names:\n",
        "    file_path = os.path.join(folder_name, file_name)\n",
        "    img = cv2.imread(file_path)\n",
        "    img_resized = cv2.resize(img, (160, 160))  # Resize image to (160, 160)        \n",
        "    embedding = DeepFace.represent(img_resized, model_name='Facenet512', enforce_detection=False)\n",
        "    embedding_vectors.append((file_name, embedding[0]['embedding']))\n",
        "    \n",
        "  return embedding_vectors\n",
        "\n",
        "\"\"\"Extract faces from the photo and save them as separate images.\"\"\"\n",
        "def extract_faces(photos_path, orig_faces_path, closeup_faces_path):\n",
        "  if not os.path.exists(closeup_faces_path):\n",
        "    os.makedirs(closeup_faces_path)\n",
        "  if not os.path.exists(orig_faces_path):\n",
        "    os.makedirs(orig_faces_path)\n",
        "\n",
        "  file_names = [file for file in os.listdir(photos_path) if os.path.isfile(os.path.join(photos_path, file))]\n",
        "  for fi in range(len(file_names)):\n",
        "    file_path = os.path.join(photos_path, file_names[fi])\n",
        "    img = cv2.imread(file_path)\n",
        "    img_height, img_width, _ = img.shape\n",
        "    print(\"Extracting Faces : %d%%\\r\" % (int((fi * 100) / len(file_names))))\n",
        "    faces = DeepFace.extract_faces(img, enforce_detection=False, align=True, detector_backend='retinaface')\n",
        "\n",
        "    for i, face in enumerate(faces):\n",
        "      # Ignore items that aren't confidently a face\n",
        "      if face['confidence'] < 0.98:\n",
        "        continue\n",
        "      # Full face images are square-ish. So if it's longer than .5 on either sides, ignore it.\n",
        "      facial_area = face['facial_area']\n",
        "      wh_ratio = facial_area['w'] / facial_area['h']\n",
        "      if wh_ratio > 1.5 or wh_ratio < 0.66:  \n",
        "        continue\n",
        "\n",
        "      # Save the close up face image\n",
        "      destpath = os.path.join(closeup_faces_path, f\"{os.path.splitext(os.path.basename(file_path))[0]}_face_{i}.jpg\")\n",
        "      cv2.imwrite(destpath, cv2.cvtColor(cv2.convertScaleAbs(face['face'], alpha=(255.0)), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      # Expand the face rectangle by 50% on all sides and save that as the 'original' image\n",
        "      expand = 0.5\n",
        "      dim = max(facial_area['w'], facial_area['h'])\n",
        "      x = max(0, facial_area['x'] - int((dim-facial_area['w'])/2) - int(dim * expand))\n",
        "      y = max(0, facial_area['y'] - int((dim-facial_area['h'])/2) - int(dim * expand))\n",
        "      w = min(img_width, int(dim * (1+expand*2)))\n",
        "      h = min(img_height, int(dim * (1+expand*2)))\n",
        "      face_img = img[y:y+h, x:x+w]\n",
        "      destpath = os.path.join(orig_faces_path, f\"{os.path.splitext(os.path.basename(file_path))[0]}_face_{i}.jpg\")\n",
        "      cv2.imwrite(destpath, face_img)\n",
        "  print(\"\\n\")\n",
        "\n",
        "def find_max_pairwise_diff_in_list(nums):\n",
        "  if len(nums) < 2:\n",
        "    return 0\n",
        "  nums.sort()  # Sort the list in ascending order\n",
        "  max_diff = float('-inf')\n",
        "  for i in range(len(nums) - 1):\n",
        "    diff = nums[i+1] - nums[i]\n",
        "    max_diff = max(max_diff, diff)\n",
        "  return max_diff\n",
        "\n",
        "def find_best_set_of_unique_faces(embeddings):\n",
        "  threshold = 0.01\n",
        "  min_diff = 1000000\n",
        "  min_diff_threshold = threshold\n",
        "  min_diff_counts = []\n",
        "  min_diff_unique_faces = []\n",
        "  while threshold < 10:\n",
        "    threshold += 0.01\n",
        "    unique_faces = find_unique_faces_dbscan(embeddings, threshold)\n",
        "    if len(unique_faces) > 1:\n",
        "      print(threshold)\n",
        "      counts=[]\n",
        "      for face in unique_faces:\n",
        "        counts.append(face['count'])\n",
        "      print(counts)\n",
        "      selected = False\n",
        "      if len(unique_faces) > len(min_diff_unique_faces):\n",
        "        selected = True\n",
        "      elif len(unique_faces) == len(min_diff_unique_faces):\n",
        "        diff = find_max_pairwise_diff_in_list(counts)\n",
        "        if diff < min_diff:\n",
        "          selected = True\n",
        "      if selected:\n",
        "        min_diff_threshold = threshold\n",
        "        min_diff = find_max_pairwise_diff_in_list(counts)\n",
        "        min_diff_counts = counts\n",
        "        min_diff_unique_faces = unique_faces\n",
        "\n",
        "  print(f\"Final threshold & face cluster counts : {min_diff_threshold}, {min_diff_counts}\")\n",
        "  return min_diff_unique_faces\n",
        "\n",
        "\"\"\" Copy the unique faces to separate directories. \"\"\"\n",
        "def organize_unique_face_photos_in_directories(unique_faces, orig_faces_dir, closeup_faces_dir):\n",
        "  for i in range(len(unique_faces)):\n",
        "    origdir = f\"{orig_faces_dir}/face-{i}\"\n",
        "    dir = f\"{closeup_faces_dir}/face-{i}\"\n",
        "    shutil.rmtree(origdir, True)\n",
        "    shutil.rmtree(dir, True)\n",
        "    os.mkdir(origdir)\n",
        "    os.mkdir(dir)\n",
        "    for name in unique_faces[i]['photos']:\n",
        "      shutil.copyfile(f\"{closeup_faces_dir}/{name}\", f\"{dir}/{name}\")\n",
        "      shutil.copyfile(f\"{orig_faces_dir}/{name}\", f\"{origdir}/{name}\")\n",
        "\n",
        "\"\"\" Show a grid of images from the given directory into `output` \"\"\"\n",
        "def show_grid_of_images(image_dir, output, num_images=8):\n",
        "  # Clear the previous images\n",
        "  output.clear_output()\n",
        "\n",
        "  # Get the list of image files\n",
        "  image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir)]\n",
        "  image_files = image_files[:num_images]  # take only the first n_images\n",
        "\n",
        "  with output:\n",
        "    # Set up the grid\n",
        "    _, axs = plt.subplots(2, 4)\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    for i, image_file in enumerate(image_files):\n",
        "      img = PIL.Image.open(image_file)\n",
        "      axs[i].imshow(img)\n",
        "      axs[i].axis('off')  # hide the axes\n",
        "\n",
        "    # If less than n_images, hide the rest of the axes\n",
        "    for j in range(i+1, num_images):\n",
        "      axs[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\"\"\" Copy the unique faces to separate directories. \"\"\"\n",
        "def copy_images_to_instance_images_dir(src_dir, instance_images_dir):\n",
        "  shutil.rmtree(instance_images_dir, True)\n",
        "  os.mkdir(instance_images_dir)\n",
        "  srcFiles = os.listdir(src_dir)\n",
        "  for filename in srcFiles:\n",
        "    filePath = os.path.join(src_dir, filename)\n",
        "    if os.path.isfile(filePath):\n",
        "      shutil.copy(filePath, instance_images_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get your photos .zip downloaded from your Google Drive\n",
        "Do you have a .zip file of your photos already saved in your Google Drive? **If not**, see the instructions at the very top of this page before continuing further."
      ],
      "metadata": {
        "id": "awb_zlXBRVzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuTiMStsRnom",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Connect & mount Google Drive\n",
        "#@markdown Run this cell to mount your Google Drive.\n",
        "gdrive_mount_path = \"/content/gdrive\" #@param {type: \"string\"}\n",
        "from google.colab import drive\n",
        "drive.mount(gdrive_mount_path, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI0vjvabPCtY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Settings\n",
        "#@markdown Setup variables for extracting photos from your Google Drive .zip files to local VM directory.\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "#@markdown `zipsDir` The directory in Google Drive where you have the photos .zip files\n",
        "zips_dir = \"photo_genie\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown `photosDir` The directory in VM to store the extracted photos\n",
        "photos_dir = \"/content/photos\" #@param {type: \"string\"}\n",
        "\n",
        "gdrive_photo_genie_path = f'{gdrive_mount_path}/MyDrive/{zips_dir}'\n",
        "orig_faces_dir = f\"{gdrive_photo_genie_path}/orig_faces\"\n",
        "closeup_faces_dir = f\"{gdrive_photo_genie_path}/faces\"\n",
        "instance_faces_dir = f\"{gdrive_photo_genie_path}/instance_images\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract photos from your Google Drive .zip files to local VM directory.\n",
        "shutil.rmtree(photos_dir, ignore_errors=True)\n",
        "os.makedirs(photos_dir, exist_ok=True)\n",
        "\n",
        "# Extract all photos from the .zips to local directory\n",
        "extract_photos_from_zips(gdrive_photo_genie_path, photos_dir)"
      ],
      "metadata": {
        "id": "nCFOT5bmMCg2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract unique faces, group them and select one group"
      ],
      "metadata": {
        "id": "4WDSOV0HSQ_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WLtvZd-sjtq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Extract faces & group them by similarity.\n",
        "extract_faces(photos_dir, orig_faces_dir, closeup_faces_dir)\n",
        "embeddings = generate_embedding_vectors(closeup_faces_dir, r'.*_face_\\d\\.jpg')\n",
        "unique_faces = find_best_set_of_unique_faces(embeddings)\n",
        "organize_unique_face_photos_in_directories(unique_faces, orig_faces_dir, closeup_faces_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQt6OwcupQNk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Select the set of faces to fine-tune the model\n",
        "from ipywidgets import Button, HBox, Output, VBox, Label\n",
        "\n",
        "# Create an Output widget for the images\n",
        "output = Output()\n",
        "\n",
        "def handle_directory_click(directory, output):\n",
        "  show_grid_of_images(f\"{closeup_faces_dir}/{directory}\", output)\n",
        "  copy_images_to_instance_images_dir(f\"{orig_faces_dir}/{directory}\", instance_faces_dir)\n",
        "\n",
        "# Create a Button widget for each directory\n",
        "buttons = []\n",
        "for i in range(len(unique_faces)):\n",
        "    directory = f\"face-{i}\"\n",
        "    button = Button(description=directory)\n",
        "    button.on_click(lambda x, directory=directory: handle_directory_click(directory, output))\n",
        "    buttons.append(button)\n",
        "\n",
        "# Create a VBox: one row for the buttons, one row for the output\n",
        "VBox([Label(value=\"Select a group below by clicking on the button\"), HBox(buttons), output])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "# Train & fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxg0y5MBudmd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Model & Weights settings\n",
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"stabilityai/stable-diffusion-2-1\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = f\"{gdrive_photo_genie_path}/{OUTPUT_DIR}\"\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "#@markdown Maximum training runs.\n",
        "MAX_TRAIN_RUNS = 800 #@param {type:\"integer\"}\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vDpCxId1aCm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Concepts - instance & class images\n",
        "# You can also add multiple concepts here.\n",
        "\n",
        "#@markdown Prompt describing your photos. `zwx` is just a unique name identifying you.\n",
        "instance_prompt = \"photo of zwx's face\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Prompt describing the general class of your photos. Edit this to best describe how you & your photos identify.\n",
        "class_prompt = \"photo of a man's face\" #@param {type:\"string\"}\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      instance_prompt,\n",
        "        \"class_prompt\":         class_prompt,\n",
        "        \"instance_data_dir\":    instance_faces_dir,\n",
        "        \"class_data_dir\":       \"/content/class_data\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clear up GPU memory before training\n",
        "!pip install numba\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device() \n",
        "device.reset()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BEeFsNi4FM42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run training & fine-tuning\n",
        "!python3 train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path={MODEL_NAME} \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir={OUTPUT_DIR} \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=50 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps={MAX_TRAIN_RUNS} \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"oil painting on canvas of zwx's face\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXzsUyG1aCy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Convert weights to ckpt to use in web UIs like AUTOMATIC1111.\n",
        "#@markdown Run conversion.\n",
        "ckpt_path = f\"{OUTPUT_DIR}/{MAX_TRAIN_RUNS}/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path \"{OUTPUT_DIR}/{MAX_TRAIN_RUNS}\"  --checkpoint_path {ckpt_path} {half_arg}\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the model to create your own images"
      ],
      "metadata": {
        "id": "Z4YHjCUyvDZT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setup inference\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "\n",
        "# If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "model_path = f\"{OUTPUT_DIR}/{MAX_TRAIN_RUNS}\"\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "#@markdown Can set random seed here for reproducibility.\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "g_cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6xoHWSsbcS3",
        "scrolled": false,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create your own images, using the fine-tuned model\n",
        "\n",
        "prompt = \"caricature pencil drawing of zwx chasing a dog\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 10 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 24 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgi8HM4c-DA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Free runtime memory\n",
        "exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLneuIl5/SK9QIsYyg6rB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}